{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4NRpYRQ2NOP",
        "outputId": "90b4d471-fffa-4234-d30c-7b7ce2103390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B77PzQj22Kjx"
      },
      "source": [
        "# Day 3: Multi-Layer Perceptrons\n",
        "\n",
        "In this lab, we aim to solve the problem of classifying human faces into males and females. We have at our disposition a dataset of 200 images of celebrity faces and their associated labels (0 for male and 1 for female).  \n",
        "\n",
        "We  use a face detector from the dlib library to  estimate  the  location  of  68  (x,  y) coordinates  that  map  to specific facial regions. The image below visualizes what each of these coordinates maps to:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs4Z9JJh2Kj2"
      },
      "source": [
        "![](face_feature_extraction.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZMif--k2Kj3"
      },
      "source": [
        "We then use tensorflow to define and train a multi-layer perceptron (MLP) graph to classify images using the features visualized above. Using the code below, try to apply the following changes:\n",
        "    \n",
        "1 - Change the complexity of the 2-layer MLP by increasing or decreasing the number of neurons in each layer.\n",
        "\n",
        "2 - Try increasing the number of layers used. This should increase the \"depth\" of the MLP. To do so, you must change the definition of the MLP function in \"multilayer_perceptron\" and the weights/biases allocation function \"allocate_weights_and_biases\".\n",
        "\n",
        "3 - Try changing how the weights and parameters are initialized. What would happen if you initialize all parameters to zero ?\n",
        "\n",
        "4 - Try increasing or decreasing the learning rate and number of training epochs. How does this affect the \"fitting\" to training data ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGdtCc2x2Kj4"
      },
      "source": [
        "### Import APIs to be used "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKktQZxo24Zo"
      },
      "outputs": [],
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "somemodule = SourceFileLoader('lab3_data', '/content/drive/MyDrive/Collab/lab7/lab3_data.py').load_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q83mH4p2Kj5",
        "outputId": "5c7e09b7-d9ff-475f-a142-a2f1e56c2ae4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import lab3_data as import_data\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeFmYYpE2Kj7"
      },
      "source": [
        "### Load  CelebA data and create train and test splits (Train: 100 exmaples, Test: 100 examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn0qNr572Kj8"
      },
      "outputs": [],
      "source": [
        "def get_data():\n",
        "    X, y = import_data.extract_features_labels()\n",
        "    Y = np.array([y, -(y - 1)]).T\n",
        "    tr_X = X[:100] ; tr_Y = Y[:100]\n",
        "    te_X = X[100:] ; te_Y = Y[100:]\n",
        "\n",
        "    return tr_X, tr_Y, te_X, te_Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "PxQd3MXo2Kj-"
      },
      "source": [
        "### Allocate memory for weights and biases for all MLP layers\n",
        "You can try changing the number of neurons to increase or decrease the complexity of the MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp8O6qU52Kj_"
      },
      "outputs": [],
      "source": [
        "def allocate_weights_and_biases():\n",
        "\n",
        "    # define number of hidden layers ..\n",
        "    n_hidden_1 = 2048  # 1st layer number of neurons\n",
        "    n_hidden_2 = 2048  # 2nd layer number of neurons\n",
        "\n",
        "    # inputs placeholders\n",
        "    X = tf.placeholder(\"float\", [None, 68, 2])\n",
        "    Y = tf.placeholder(\"float\", [None, 2])  # 2 output classes\n",
        "    \n",
        "    # flatten image features into one vector (i.e. reshape image feature matrix into a vector)\n",
        "    # images_flat = tf.contrib.layers.flatten(X)  \n",
        "    images_flat = tf.compat.v1.layers.flatten(X)  \n",
        "    \n",
        "    \n",
        "    # weights and biases are initialized from a normal distribution with a specified standard devation stddev\n",
        "    stddev = 0.01\n",
        "    \n",
        "    # define placeholders for weights and biases in the graph\n",
        "    weights = {\n",
        "        'hidden_layer1': tf.Variable(tf.random_normal([68 * 2, n_hidden_1], stddev=stddev)),\n",
        "        'hidden_layer2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
        "        'out': tf.Variable(tf.random_normal([n_hidden_2, 2], stddev=stddev))\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "        'bias_layer1': tf.Variable(tf.random_normal([n_hidden_1], stddev=stddev)),\n",
        "        'bias_layer2': tf.Variable(tf.random_normal([n_hidden_2], stddev=stddev)),\n",
        "        'out': tf.Variable(tf.random_normal([2], stddev=stddev))\n",
        "    }\n",
        "    \n",
        "    return weights, biases, X, Y, images_flat\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "unLUr4n22KkB"
      },
      "source": [
        "### Define how the weights and biases are used for inferring classes from inputs (i.e. define MLP function)\n",
        "\n",
        "You can add more layers to the MLP to fit more complicated functions. Adding more layers requires more learnable weights and biases, which need to defined in \"allocate_weights_and_biases\" first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP0jETlw2KkC",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "def multilayer_perceptron():\n",
        "        \n",
        "    weights, biases, X, Y, images_flat = allocate_weights_and_biases()\n",
        "\n",
        "    # Hidden fully connected layer 1\n",
        "    layer_1 = tf.add(tf.matmul(images_flat, weights['hidden_layer1']), biases['bias_layer1'])\n",
        "    layer_1 = tf.sigmoid(layer_1)\n",
        "\n",
        "    # Hidden fully connected layer 2\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer2']), biases['bias_layer2'])\n",
        "    layer_2 = tf.sigmoid(layer_2)\n",
        "    \n",
        "    # Output fully connected layer\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "\n",
        "    return out_layer, X, Y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs1HW7vi2KkD"
      },
      "source": [
        "### Define graph training operation\n",
        "The loss function (i.e. the value to minimize) is defined as the cross entropy between the predicted classes and the class ground truth. The train operation is then included within the graph as a weight/bias update operation.\n",
        "\n",
        "Try changing the learning rate, how would setting a low or high learning rate affect the \"fitting\" to the training set ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_jdyKWH2KkD",
        "outputId": "3a7aed81-cf45-4070-8642-b3ba136cdce4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  del sys.path[0]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# learning parameters\n",
        "learning_rate = 1e-5\n",
        "training_epochs = 500\n",
        "\n",
        "# display training accuracy every ..\n",
        "display_accuracy_step = 10\n",
        "\n",
        "    \n",
        "training_images, training_labels, test_images, test_labels = get_data()\n",
        "logits, X, Y = multilayer_perceptron()\n",
        "\n",
        "# define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# define training graph operation\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# graph operation to initialize all variables\n",
        "init_op = tf.global_variables_initializer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4rKbJUT2KkE"
      },
      "source": [
        "### Run graph for specified number of epochs.\n",
        "\n",
        "After the graph is defined, different operations in the graph can be run by specifying them in the sess.run() function.\n",
        "A session is wrapper for running graphs. Outputs can also be acquired from the graph by including them in the variable list of sess.run()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP1PCFlK8wQJ",
        "outputId": "3e0901ff-d275-4b5a-95f9-d834886c31aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost=0.695002615\n",
            "Accuracy: 0.520\n",
            "Epoch: 0002 cost=0.692600727\n",
            "Epoch: 0003 cost=0.691955388\n",
            "Epoch: 0004 cost=0.692250371\n",
            "Epoch: 0005 cost=0.692385733\n",
            "Epoch: 0006 cost=0.692014456\n",
            "Epoch: 0007 cost=0.691375375\n",
            "Epoch: 0008 cost=0.690776348\n",
            "Epoch: 0009 cost=0.690397859\n",
            "Epoch: 0010 cost=0.690240622\n",
            "Epoch: 0011 cost=0.690162063\n",
            "Accuracy: 0.520\n",
            "Epoch: 0012 cost=0.689997196\n",
            "Epoch: 0013 cost=0.689676762\n",
            "Epoch: 0014 cost=0.689244986\n",
            "Epoch: 0015 cost=0.688800454\n",
            "Epoch: 0016 cost=0.688426375\n",
            "Epoch: 0017 cost=0.688143253\n",
            "Epoch: 0018 cost=0.687904537\n",
            "Epoch: 0019 cost=0.687637806\n",
            "Epoch: 0020 cost=0.687299430\n",
            "Epoch: 0021 cost=0.686902642\n",
            "Accuracy: 0.530\n",
            "Epoch: 0022 cost=0.686499417\n",
            "Epoch: 0023 cost=0.686137080\n",
            "Epoch: 0024 cost=0.685822546\n",
            "Epoch: 0025 cost=0.685518980\n",
            "Epoch: 0026 cost=0.685180485\n",
            "Epoch: 0027 cost=0.684790730\n",
            "Epoch: 0028 cost=0.684371948\n",
            "Epoch: 0029 cost=0.683958530\n",
            "Epoch: 0030 cost=0.683564663\n",
            "Epoch: 0031 cost=0.683175445\n",
            "Accuracy: 0.600\n",
            "Epoch: 0032 cost=0.682766676\n",
            "Epoch: 0033 cost=0.682332695\n",
            "Epoch: 0034 cost=0.681890011\n",
            "Epoch: 0035 cost=0.681455553\n",
            "Epoch: 0036 cost=0.681024849\n",
            "Epoch: 0037 cost=0.680577695\n",
            "Epoch: 0038 cost=0.680103660\n",
            "Epoch: 0039 cost=0.679614186\n",
            "Epoch: 0040 cost=0.679127514\n",
            "Epoch: 0041 cost=0.678648531\n",
            "Accuracy: 0.630\n",
            "Epoch: 0042 cost=0.678169727\n",
            "Epoch: 0043 cost=0.677687824\n",
            "Epoch: 0044 cost=0.677210748\n",
            "Epoch: 0045 cost=0.676739872\n",
            "Epoch: 0046 cost=0.676263630\n",
            "Epoch: 0047 cost=0.675774336\n",
            "Epoch: 0048 cost=0.675279140\n",
            "Epoch: 0049 cost=0.674786747\n",
            "Epoch: 0050 cost=0.674292743\n",
            "Epoch: 0051 cost=0.673788905\n",
            "Accuracy: 0.630\n",
            "Epoch: 0052 cost=0.673273683\n",
            "Epoch: 0053 cost=0.672746301\n",
            "Epoch: 0054 cost=0.672199547\n",
            "Epoch: 0055 cost=0.671633601\n",
            "Epoch: 0056 cost=0.671057224\n",
            "Epoch: 0057 cost=0.670476139\n",
            "Epoch: 0058 cost=0.669890761\n",
            "Epoch: 0059 cost=0.669304490\n",
            "Epoch: 0060 cost=0.668719947\n",
            "Epoch: 0061 cost=0.668132544\n",
            "Accuracy: 0.650\n",
            "Epoch: 0062 cost=0.667540669\n",
            "Epoch: 0063 cost=0.666944742\n",
            "Epoch: 0064 cost=0.666340113\n",
            "Epoch: 0065 cost=0.665724277\n",
            "Epoch: 0066 cost=0.665097117\n",
            "Epoch: 0067 cost=0.664455473\n",
            "Epoch: 0068 cost=0.663801432\n",
            "Epoch: 0069 cost=0.663141072\n",
            "Epoch: 0070 cost=0.662477314\n",
            "Epoch: 0071 cost=0.661814213\n",
            "Accuracy: 0.640\n",
            "Epoch: 0072 cost=0.661152124\n",
            "Epoch: 0073 cost=0.660486042\n",
            "Epoch: 0074 cost=0.659813106\n",
            "Epoch: 0075 cost=0.659128189\n",
            "Epoch: 0076 cost=0.658427894\n",
            "Epoch: 0077 cost=0.657711625\n",
            "Epoch: 0078 cost=0.656978250\n",
            "Epoch: 0079 cost=0.656229317\n",
            "Epoch: 0080 cost=0.655466020\n",
            "Epoch: 0081 cost=0.654691935\n",
            "Accuracy: 0.660\n",
            "Epoch: 0082 cost=0.653910697\n",
            "Epoch: 0083 cost=0.653124869\n",
            "Epoch: 0084 cost=0.652334571\n",
            "Epoch: 0085 cost=0.651537478\n",
            "Epoch: 0086 cost=0.650731027\n",
            "Epoch: 0087 cost=0.649912953\n",
            "Epoch: 0088 cost=0.649083316\n",
            "Epoch: 0089 cost=0.648241818\n",
            "Epoch: 0090 cost=0.647388160\n",
            "Epoch: 0091 cost=0.646523416\n",
            "Accuracy: 0.690\n",
            "Epoch: 0092 cost=0.645651877\n",
            "Epoch: 0093 cost=0.644778430\n",
            "Epoch: 0094 cost=0.643903673\n",
            "Epoch: 0095 cost=0.643022895\n",
            "Epoch: 0096 cost=0.642129660\n",
            "Epoch: 0097 cost=0.641219318\n",
            "Epoch: 0098 cost=0.640291929\n",
            "Epoch: 0099 cost=0.639350533\n",
            "Epoch: 0100 cost=0.638399124\n",
            "Epoch: 0101 cost=0.637439370\n",
            "Accuracy: 0.680\n",
            "Epoch: 0102 cost=0.636471689\n",
            "Epoch: 0103 cost=0.635497451\n",
            "Epoch: 0104 cost=0.634518206\n",
            "Epoch: 0105 cost=0.633534372\n",
            "Epoch: 0106 cost=0.632544994\n",
            "Epoch: 0107 cost=0.631549120\n",
            "Epoch: 0108 cost=0.630545735\n",
            "Epoch: 0109 cost=0.629533947\n",
            "Epoch: 0110 cost=0.628513396\n",
            "Epoch: 0111 cost=0.627485096\n",
            "Accuracy: 0.680\n",
            "Epoch: 0112 cost=0.626451612\n",
            "Epoch: 0113 cost=0.625414968\n",
            "Epoch: 0114 cost=0.624375343\n",
            "Epoch: 0115 cost=0.623331845\n",
            "Epoch: 0116 cost=0.622281969\n",
            "Epoch: 0117 cost=0.621222913\n",
            "Epoch: 0118 cost=0.620153785\n",
            "Epoch: 0119 cost=0.619076490\n",
            "Epoch: 0120 cost=0.617997229\n",
            "Epoch: 0121 cost=0.616932094\n",
            "Accuracy: 0.680\n",
            "Epoch: 0122 cost=0.615916491\n",
            "Epoch: 0123 cost=0.614900768\n",
            "Epoch: 0124 cost=0.613807797\n",
            "Epoch: 0125 cost=0.612701058\n",
            "Epoch: 0126 cost=0.611720085\n",
            "Epoch: 0127 cost=0.610741019\n",
            "Epoch: 0128 cost=0.609648645\n",
            "Epoch: 0129 cost=0.608592272\n",
            "Epoch: 0130 cost=0.607616723\n",
            "Epoch: 0131 cost=0.606586039\n",
            "Accuracy: 0.660\n",
            "Epoch: 0132 cost=0.605510235\n",
            "Epoch: 0133 cost=0.604496896\n",
            "Epoch: 0134 cost=0.603512168\n",
            "Epoch: 0135 cost=0.602472782\n",
            "Epoch: 0136 cost=0.601409495\n",
            "Epoch: 0137 cost=0.600387096\n",
            "Epoch: 0138 cost=0.599378526\n",
            "Epoch: 0139 cost=0.598335505\n",
            "Epoch: 0140 cost=0.597268701\n",
            "Epoch: 0141 cost=0.596223354\n",
            "Accuracy: 0.670\n",
            "Epoch: 0142 cost=0.595203996\n",
            "Epoch: 0143 cost=0.594182253\n",
            "Epoch: 0144 cost=0.593141198\n",
            "Epoch: 0145 cost=0.592086434\n",
            "Epoch: 0146 cost=0.591034830\n",
            "Epoch: 0147 cost=0.589993715\n",
            "Epoch: 0148 cost=0.588959873\n",
            "Epoch: 0149 cost=0.587929726\n",
            "Epoch: 0150 cost=0.586903751\n",
            "Epoch: 0151 cost=0.585883319\n",
            "Accuracy: 0.690\n",
            "Epoch: 0152 cost=0.584865808\n",
            "Epoch: 0153 cost=0.583841383\n",
            "Epoch: 0154 cost=0.582801342\n",
            "Epoch: 0155 cost=0.581745088\n",
            "Epoch: 0156 cost=0.580687642\n",
            "Epoch: 0157 cost=0.579643905\n",
            "Epoch: 0158 cost=0.578616261\n",
            "Epoch: 0159 cost=0.577602148\n",
            "Epoch: 0160 cost=0.576602161\n",
            "Epoch: 0161 cost=0.575622380\n",
            "Accuracy: 0.680\n",
            "Epoch: 0162 cost=0.574658930\n",
            "Epoch: 0163 cost=0.573691130\n",
            "Epoch: 0164 cost=0.572662115\n",
            "Epoch: 0165 cost=0.571578979\n",
            "Epoch: 0166 cost=0.570527613\n",
            "Epoch: 0167 cost=0.569561779\n",
            "Epoch: 0168 cost=0.568643153\n",
            "Epoch: 0169 cost=0.567702770\n",
            "Epoch: 0170 cost=0.566695690\n",
            "Epoch: 0171 cost=0.565649092\n",
            "Accuracy: 0.710\n",
            "Epoch: 0172 cost=0.564635217\n",
            "Epoch: 0173 cost=0.563684583\n",
            "Epoch: 0174 cost=0.562767386\n",
            "Epoch: 0175 cost=0.561839223\n",
            "Epoch: 0176 cost=0.560870528\n",
            "Epoch: 0177 cost=0.559863567\n",
            "Epoch: 0178 cost=0.558852792\n",
            "Epoch: 0179 cost=0.557870269\n",
            "Epoch: 0180 cost=0.556920469\n",
            "Epoch: 0181 cost=0.555991888\n",
            "Accuracy: 0.710\n",
            "Epoch: 0182 cost=0.555073142\n",
            "Epoch: 0183 cost=0.554155052\n",
            "Epoch: 0184 cost=0.553222179\n",
            "Epoch: 0185 cost=0.552259266\n",
            "Epoch: 0186 cost=0.551268816\n",
            "Epoch: 0187 cost=0.550275505\n",
            "Epoch: 0188 cost=0.549304366\n",
            "Epoch: 0189 cost=0.548361957\n",
            "Epoch: 0190 cost=0.547441602\n",
            "Epoch: 0191 cost=0.546538115\n",
            "Accuracy: 0.720\n",
            "Epoch: 0192 cost=0.545654476\n",
            "Epoch: 0193 cost=0.544786930\n",
            "Epoch: 0194 cost=0.543916106\n",
            "Epoch: 0195 cost=0.542991459\n",
            "Epoch: 0196 cost=0.541988730\n",
            "Epoch: 0197 cost=0.540966809\n",
            "Epoch: 0198 cost=0.540014267\n",
            "Epoch: 0199 cost=0.539144933\n",
            "Epoch: 0200 cost=0.538310111\n",
            "Epoch: 0201 cost=0.537457049\n",
            "Accuracy: 0.730\n",
            "Epoch: 0202 cost=0.536542177\n",
            "Epoch: 0203 cost=0.535572648\n",
            "Epoch: 0204 cost=0.534599483\n",
            "Epoch: 0205 cost=0.533677816\n",
            "Epoch: 0206 cost=0.532811880\n",
            "Epoch: 0207 cost=0.531972885\n",
            "Epoch: 0208 cost=0.531133056\n",
            "Epoch: 0209 cost=0.530270100\n",
            "Epoch: 0210 cost=0.529370487\n",
            "Epoch: 0211 cost=0.528438747\n",
            "Accuracy: 0.740\n",
            "Epoch: 0212 cost=0.527501881\n",
            "Epoch: 0213 cost=0.526587427\n",
            "Epoch: 0214 cost=0.525704622\n",
            "Epoch: 0215 cost=0.524845898\n",
            "Epoch: 0216 cost=0.524003625\n",
            "Epoch: 0217 cost=0.523179710\n",
            "Epoch: 0218 cost=0.522376299\n",
            "Epoch: 0219 cost=0.521590590\n",
            "Epoch: 0220 cost=0.520784497\n",
            "Epoch: 0221 cost=0.519899845\n",
            "Accuracy: 0.760\n",
            "Epoch: 0222 cost=0.518923998\n",
            "Epoch: 0223 cost=0.517944932\n",
            "Epoch: 0224 cost=0.517061710\n",
            "Epoch: 0225 cost=0.516280591\n",
            "Epoch: 0226 cost=0.515538394\n",
            "Epoch: 0227 cost=0.514764428\n",
            "Epoch: 0228 cost=0.513909757\n",
            "Epoch: 0229 cost=0.512986362\n",
            "Epoch: 0230 cost=0.512060106\n",
            "Epoch: 0231 cost=0.511193871\n",
            "Accuracy: 0.780\n",
            "Epoch: 0232 cost=0.510392189\n",
            "Epoch: 0233 cost=0.509623170\n",
            "Epoch: 0234 cost=0.508854330\n",
            "Epoch: 0235 cost=0.508061171\n",
            "Epoch: 0236 cost=0.507225454\n",
            "Epoch: 0237 cost=0.506349921\n",
            "Epoch: 0238 cost=0.505455196\n",
            "Epoch: 0239 cost=0.504578888\n",
            "Epoch: 0240 cost=0.503736973\n",
            "Epoch: 0241 cost=0.502928495\n",
            "Accuracy: 0.790\n",
            "Epoch: 0242 cost=0.502145946\n",
            "Epoch: 0243 cost=0.501386344\n",
            "Epoch: 0244 cost=0.500653505\n",
            "Epoch: 0245 cost=0.499945194\n",
            "Epoch: 0246 cost=0.499239206\n",
            "Epoch: 0247 cost=0.498479038\n",
            "Epoch: 0248 cost=0.497598648\n",
            "Epoch: 0249 cost=0.496626705\n",
            "Epoch: 0250 cost=0.495691001\n",
            "Epoch: 0251 cost=0.494889379\n",
            "Accuracy: 0.780\n",
            "Epoch: 0252 cost=0.494200855\n",
            "Epoch: 0253 cost=0.493540078\n",
            "Epoch: 0254 cost=0.492826045\n",
            "Epoch: 0255 cost=0.492010921\n",
            "Epoch: 0256 cost=0.491120428\n",
            "Epoch: 0257 cost=0.490240365\n",
            "Epoch: 0258 cost=0.489436299\n",
            "Epoch: 0259 cost=0.488708109\n",
            "Epoch: 0260 cost=0.488018185\n",
            "Epoch: 0261 cost=0.487323761\n",
            "Accuracy: 0.800\n",
            "Epoch: 0262 cost=0.486594766\n",
            "Epoch: 0263 cost=0.485814869\n",
            "Epoch: 0264 cost=0.484988868\n",
            "Epoch: 0265 cost=0.484148800\n",
            "Epoch: 0266 cost=0.483330756\n",
            "Epoch: 0267 cost=0.482551038\n",
            "Epoch: 0268 cost=0.481805682\n",
            "Epoch: 0269 cost=0.481085062\n",
            "Epoch: 0270 cost=0.480383098\n",
            "Epoch: 0271 cost=0.479704320\n",
            "Accuracy: 0.800\n",
            "Epoch: 0272 cost=0.479054809\n",
            "Epoch: 0273 cost=0.478421748\n",
            "Epoch: 0274 cost=0.477766901\n",
            "Epoch: 0275 cost=0.477013171\n",
            "Epoch: 0276 cost=0.476126492\n",
            "Epoch: 0277 cost=0.475186080\n",
            "Epoch: 0278 cost=0.474335432\n",
            "Epoch: 0279 cost=0.473634154\n",
            "Epoch: 0280 cost=0.473030508\n",
            "Epoch: 0281 cost=0.472432733\n",
            "Accuracy: 0.800\n",
            "Epoch: 0282 cost=0.471759886\n",
            "Epoch: 0283 cost=0.470978558\n",
            "Epoch: 0284 cost=0.470126599\n",
            "Epoch: 0285 cost=0.469294101\n",
            "Epoch: 0286 cost=0.468542278\n",
            "Epoch: 0287 cost=0.467869908\n",
            "Epoch: 0288 cost=0.467238784\n",
            "Epoch: 0289 cost=0.466609657\n",
            "Epoch: 0290 cost=0.465948462\n",
            "Epoch: 0291 cost=0.465235412\n",
            "Accuracy: 0.830\n",
            "Epoch: 0292 cost=0.464467466\n",
            "Epoch: 0293 cost=0.463670909\n",
            "Epoch: 0294 cost=0.462881356\n",
            "Epoch: 0295 cost=0.462127656\n",
            "Epoch: 0296 cost=0.461413115\n",
            "Epoch: 0297 cost=0.460728407\n",
            "Epoch: 0298 cost=0.460064858\n",
            "Epoch: 0299 cost=0.459420592\n",
            "Epoch: 0300 cost=0.458805084\n",
            "Epoch: 0301 cost=0.458227754\n",
            "Accuracy: 0.810\n",
            "Epoch: 0302 cost=0.457688719\n",
            "Epoch: 0303 cost=0.457139313\n",
            "Epoch: 0304 cost=0.456478685\n",
            "Epoch: 0305 cost=0.455615580\n",
            "Epoch: 0306 cost=0.454626679\n",
            "Epoch: 0307 cost=0.453734398\n",
            "Epoch: 0308 cost=0.453069985\n",
            "Epoch: 0309 cost=0.452566504\n",
            "Epoch: 0310 cost=0.452063203\n",
            "Epoch: 0311 cost=0.451424718\n",
            "Accuracy: 0.830\n",
            "Epoch: 0312 cost=0.450623423\n",
            "Epoch: 0313 cost=0.449761689\n",
            "Epoch: 0314 cost=0.448987186\n",
            "Epoch: 0315 cost=0.448352218\n",
            "Epoch: 0316 cost=0.447803110\n",
            "Epoch: 0317 cost=0.447250456\n",
            "Epoch: 0318 cost=0.446625978\n",
            "Epoch: 0319 cost=0.445910603\n",
            "Epoch: 0320 cost=0.445140481\n",
            "Epoch: 0321 cost=0.444380790\n",
            "Accuracy: 0.840\n",
            "Epoch: 0322 cost=0.443677545\n",
            "Epoch: 0323 cost=0.443036079\n",
            "Epoch: 0324 cost=0.442435443\n",
            "Epoch: 0325 cost=0.441852063\n",
            "Epoch: 0326 cost=0.441269308\n",
            "Epoch: 0327 cost=0.440676093\n",
            "Epoch: 0328 cost=0.440062404\n",
            "Epoch: 0329 cost=0.439417839\n",
            "Epoch: 0330 cost=0.438736647\n",
            "Epoch: 0331 cost=0.438023537\n",
            "Accuracy: 0.830\n",
            "Epoch: 0332 cost=0.437295258\n",
            "Epoch: 0333 cost=0.436574399\n",
            "Epoch: 0334 cost=0.435875714\n",
            "Epoch: 0335 cost=0.435205460\n",
            "Epoch: 0336 cost=0.434559911\n",
            "Epoch: 0337 cost=0.433930844\n",
            "Epoch: 0338 cost=0.433315933\n",
            "Epoch: 0339 cost=0.432722360\n",
            "Epoch: 0340 cost=0.432167917\n",
            "Epoch: 0341 cost=0.431688190\n",
            "Accuracy: 0.840\n",
            "Epoch: 0342 cost=0.431329191\n",
            "Epoch: 0343 cost=0.431122512\n",
            "Epoch: 0344 cost=0.430938154\n",
            "Epoch: 0345 cost=0.430374146\n",
            "Epoch: 0346 cost=0.429131955\n",
            "Epoch: 0347 cost=0.427747458\n",
            "Epoch: 0348 cost=0.427043557\n",
            "Epoch: 0349 cost=0.426952720\n",
            "Epoch: 0350 cost=0.426702946\n",
            "Epoch: 0351 cost=0.425809413\n",
            "Accuracy: 0.840\n",
            "Epoch: 0352 cost=0.424700618\n",
            "Epoch: 0353 cost=0.424066037\n",
            "Epoch: 0354 cost=0.423836440\n",
            "Epoch: 0355 cost=0.423417658\n",
            "Epoch: 0356 cost=0.422570407\n",
            "Epoch: 0357 cost=0.421681404\n",
            "Epoch: 0358 cost=0.421132952\n",
            "Epoch: 0359 cost=0.420784831\n",
            "Epoch: 0360 cost=0.420277178\n",
            "Epoch: 0361 cost=0.419512182\n",
            "Accuracy: 0.840\n",
            "Epoch: 0362 cost=0.418736339\n",
            "Epoch: 0363 cost=0.418160826\n",
            "Epoch: 0364 cost=0.417722404\n",
            "Epoch: 0365 cost=0.417224228\n",
            "Epoch: 0366 cost=0.416572988\n",
            "Epoch: 0367 cost=0.415853739\n",
            "Epoch: 0368 cost=0.415201455\n",
            "Epoch: 0369 cost=0.414658785\n",
            "Epoch: 0370 cost=0.414164931\n",
            "Epoch: 0371 cost=0.413638979\n",
            "Accuracy: 0.840\n",
            "Epoch: 0372 cost=0.413041979\n",
            "Epoch: 0373 cost=0.412395477\n",
            "Epoch: 0374 cost=0.411749393\n",
            "Epoch: 0375 cost=0.411138982\n",
            "Epoch: 0376 cost=0.410569161\n",
            "Epoch: 0377 cost=0.410025209\n",
            "Epoch: 0378 cost=0.409488589\n",
            "Epoch: 0379 cost=0.408948213\n",
            "Epoch: 0380 cost=0.408398658\n",
            "Epoch: 0381 cost=0.407834291\n",
            "Accuracy: 0.850\n",
            "Epoch: 0382 cost=0.407258630\n",
            "Epoch: 0383 cost=0.406674564\n",
            "Epoch: 0384 cost=0.406090885\n",
            "Epoch: 0385 cost=0.405509919\n",
            "Epoch: 0386 cost=0.404933929\n",
            "Epoch: 0387 cost=0.404364586\n",
            "Epoch: 0388 cost=0.403805047\n",
            "Epoch: 0389 cost=0.403259546\n",
            "Epoch: 0390 cost=0.402733296\n",
            "Epoch: 0391 cost=0.402234733\n",
            "Accuracy: 0.850\n",
            "Epoch: 0392 cost=0.401772320\n",
            "Epoch: 0393 cost=0.401348382\n",
            "Epoch: 0394 cost=0.400949717\n",
            "Epoch: 0395 cost=0.400528461\n",
            "Epoch: 0396 cost=0.400002092\n",
            "Epoch: 0397 cost=0.399301440\n",
            "Epoch: 0398 cost=0.398454070\n",
            "Epoch: 0399 cost=0.397604227\n",
            "Epoch: 0400 cost=0.396902353\n",
            "Epoch: 0401 cost=0.396389812\n",
            "Accuracy: 0.840\n",
            "Epoch: 0402 cost=0.396004766\n",
            "Epoch: 0403 cost=0.395650178\n",
            "Epoch: 0404 cost=0.395239681\n",
            "Epoch: 0405 cost=0.394714206\n",
            "Epoch: 0406 cost=0.394065708\n",
            "Epoch: 0407 cost=0.393346250\n",
            "Epoch: 0408 cost=0.392642796\n",
            "Epoch: 0409 cost=0.392018765\n",
            "Epoch: 0410 cost=0.391486704\n",
            "Epoch: 0411 cost=0.391022831\n",
            "Accuracy: 0.850\n",
            "Epoch: 0412 cost=0.390595526\n",
            "Epoch: 0413 cost=0.390180737\n",
            "Epoch: 0414 cost=0.389752924\n",
            "Epoch: 0415 cost=0.389294207\n",
            "Epoch: 0416 cost=0.388783008\n",
            "Epoch: 0417 cost=0.388208419\n",
            "Epoch: 0418 cost=0.387576103\n",
            "Epoch: 0419 cost=0.386913806\n",
            "Epoch: 0420 cost=0.386258662\n",
            "Epoch: 0421 cost=0.385641932\n",
            "Accuracy: 0.850\n",
            "Epoch: 0422 cost=0.385072738\n",
            "Epoch: 0423 cost=0.384547502\n",
            "Epoch: 0424 cost=0.384057015\n",
            "Epoch: 0425 cost=0.383594215\n",
            "Epoch: 0426 cost=0.383158296\n",
            "Epoch: 0427 cost=0.382759541\n",
            "Epoch: 0428 cost=0.382415384\n",
            "Epoch: 0429 cost=0.382142216\n",
            "Epoch: 0430 cost=0.381929845\n",
            "Epoch: 0431 cost=0.381694257\n",
            "Accuracy: 0.850\n",
            "Epoch: 0432 cost=0.381264657\n",
            "Epoch: 0433 cost=0.380480379\n",
            "Epoch: 0434 cost=0.379426599\n",
            "Epoch: 0435 cost=0.378451914\n",
            "Epoch: 0436 cost=0.377844542\n",
            "Epoch: 0437 cost=0.377582163\n",
            "Epoch: 0438 cost=0.377423674\n",
            "Epoch: 0439 cost=0.377109289\n",
            "Epoch: 0440 cost=0.376505196\n",
            "Epoch: 0441 cost=0.375700653\n",
            "Accuracy: 0.850\n",
            "Epoch: 0442 cost=0.374933422\n",
            "Epoch: 0443 cost=0.374379724\n",
            "Epoch: 0444 cost=0.374025762\n",
            "Epoch: 0445 cost=0.373730659\n",
            "Epoch: 0446 cost=0.373354733\n",
            "Epoch: 0447 cost=0.372829080\n",
            "Epoch: 0448 cost=0.372188985\n",
            "Epoch: 0449 cost=0.371530265\n",
            "Epoch: 0450 cost=0.370943636\n",
            "Epoch: 0451 cost=0.370456815\n",
            "Accuracy: 0.850\n",
            "Epoch: 0452 cost=0.370044023\n",
            "Epoch: 0453 cost=0.369660020\n",
            "Epoch: 0454 cost=0.369263291\n",
            "Epoch: 0455 cost=0.368829608\n",
            "Epoch: 0456 cost=0.368344069\n",
            "Epoch: 0457 cost=0.367816061\n",
            "Epoch: 0458 cost=0.367255658\n",
            "Epoch: 0459 cost=0.366690516\n",
            "Epoch: 0460 cost=0.366136551\n",
            "Epoch: 0461 cost=0.365606159\n",
            "Accuracy: 0.860\n",
            "Epoch: 0462 cost=0.365096271\n",
            "Epoch: 0463 cost=0.364600360\n",
            "Epoch: 0464 cost=0.364112318\n",
            "Epoch: 0465 cost=0.363629311\n",
            "Epoch: 0466 cost=0.363151997\n",
            "Epoch: 0467 cost=0.362685174\n",
            "Epoch: 0468 cost=0.362236142\n",
            "Epoch: 0469 cost=0.361821204\n",
            "Epoch: 0470 cost=0.361473441\n",
            "Epoch: 0471 cost=0.361267507\n",
            "Accuracy: 0.860\n",
            "Epoch: 0472 cost=0.361338317\n",
            "Epoch: 0473 cost=0.361852288\n",
            "Epoch: 0474 cost=0.362748235\n",
            "Epoch: 0475 cost=0.363116771\n",
            "Epoch: 0476 cost=0.361549973\n",
            "Epoch: 0477 cost=0.358743638\n",
            "Epoch: 0478 cost=0.357560545\n",
            "Epoch: 0479 cost=0.358487546\n",
            "Epoch: 0480 cost=0.358988613\n",
            "Epoch: 0481 cost=0.357476741\n",
            "Accuracy: 0.870\n",
            "Epoch: 0482 cost=0.355802387\n",
            "Epoch: 0483 cost=0.355881244\n",
            "Epoch: 0484 cost=0.356393456\n",
            "Epoch: 0485 cost=0.355517685\n",
            "Epoch: 0486 cost=0.354141265\n",
            "Epoch: 0487 cost=0.353904456\n",
            "Epoch: 0488 cost=0.354171336\n",
            "Epoch: 0489 cost=0.353551060\n",
            "Epoch: 0490 cost=0.352461666\n",
            "Epoch: 0491 cost=0.352084160\n",
            "Accuracy: 0.850\n",
            "Epoch: 0492 cost=0.352147609\n",
            "Epoch: 0493 cost=0.351678967\n",
            "Epoch: 0494 cost=0.350802302\n",
            "Epoch: 0495 cost=0.350316197\n",
            "Epoch: 0496 cost=0.350216776\n",
            "Epoch: 0497 cost=0.349870116\n",
            "Epoch: 0498 cost=0.349169195\n",
            "Epoch: 0499 cost=0.348599434\n",
            "Epoch: 0500 cost=0.348348528\n",
            "Optimization Finished!\n",
            "Test Accuracy: 0.8611111\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess:\n",
        "\n",
        "        # run graph weights/biases initialization op\n",
        "        sess.run(init_op)\n",
        "        # begin training loop ..\n",
        "        for epoch in range(training_epochs):\n",
        "            # complete code below\n",
        "            # run optimization operation (backprop) and cost operation (to get loss value)\n",
        "            _, cost = sess.run([train_op, loss_op], feed_dict={X: training_images,\n",
        "                                                               Y: training_labels})\n",
        "\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(cost))\n",
        "                \n",
        "            if epoch % display_accuracy_step == 0:\n",
        "                pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
        "\n",
        "                # calculate training accuracy\n",
        "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "                print(\"Accuracy: {:.3f}\".format(accuracy.eval({X: training_images, Y: training_labels})))\n",
        "\n",
        "        print(\"Optimization Finished!\")\n",
        "\n",
        "        # -- Define and run test operation -- #\n",
        "        \n",
        "        # apply softmax to output logits\n",
        "        pred = tf.nn.softmax(logits)\n",
        "        \n",
        "        #  derive inffered calasses as the class with the top value in the output density function\n",
        "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
        "        \n",
        "        # calculate accuracy\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "            \n",
        "        # complete code below\n",
        "        # run test accuracy operation ..\n",
        "        print(\"Test Accuracy:\", accuracy.eval({X: test_images, Y: test_labels}))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPUpf4SJ2KkF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('ucl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff91a95f31a462cf1f59caf2b26a13236b59a80ae8da1223e25e61ecf8f6dd53"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
